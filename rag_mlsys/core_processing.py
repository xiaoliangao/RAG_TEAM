# core_processing.py
import os
import re
from typing import List, Optional
from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

def process_single_pdf(pdf_path: str, source_name: Optional[str] = None) -> List[Document]:
    """
    å¤„ç†å•ä¸ªPDFæ–‡ä»¶ï¼ˆæ ¸å¿ƒå‡½æ•°ï¼Œç”¨äºåŠ¨æ€å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼‰
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„ 
        source_name: æ¥æºåç§°ï¼ˆç”¨äºå…ƒæ•°æ®æ ‡è®°ï¼‰
    
    Returns:
        List[Document]: å¤„ç†åçš„çŸ¥è¯†å—åˆ—è¡¨
    """
    if source_name is None:
        source_name = os.path.basename(pdf_path)
    
    print(f"æ­£åœ¨å¤„ç†: {source_name}")
    
    try:
        # 1. åŠ è½½PDF
        loader = PyMuPDFLoader(pdf_path)
        documents = loader.load()
        print(f"  âœ“ æˆåŠŸåŠ è½½ {len(documents)} ä¸ªé¡µé¢")
        
        # 2. æ¸…æ´—æ–‡æ¡£
        cleaned_documents = clean_document_content(documents)
        print(f"  âœ“ æ¸…æ´—å®Œæˆï¼Œä¿ç•™ {len(cleaned_documents)} ä¸ªæœ‰æ•ˆé¡µé¢")
        
        # 3. æ–‡æœ¬åˆ†å—
        all_chunks = split_text_into_chunks(cleaned_documents)
        print(f"  âœ“ åˆ†å—å®Œæˆï¼Œç”Ÿæˆ {len(all_chunks)} ä¸ªçŸ¥è¯†ç‰‡æ®µ")
        
        # 4. æ·»åŠ æ¥æºå…ƒæ•°æ®ï¼ˆå…³é”®ï¼šç”¨äºåç»­è¿½è¸ªï¼‰
        for chunk in all_chunks:
            chunk.metadata['source'] = source_name
            chunk.metadata['original_path'] = pdf_path
        
        return all_chunks
        
    except Exception as e:
        print(f"  âœ— å¤„ç†å¤±è´¥: {e}")
        return []


def process_directory(directory_path: str) -> List[Document]:
    """
    æ‰¹é‡å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰PDFï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰
    
    Args:
        directory_path: çŸ¥è¯†åº“ç›®å½•è·¯å¾„
    
    Returns:
        List[Document]: æ‰€æœ‰æ–‡ä»¶çš„çŸ¥è¯†å—åˆ—è¡¨
    """
    print(f"æ­£åœ¨ä» '{directory_path}' æ‰¹é‡åŠ è½½PDFæ–‡ä»¶...")
    
    # ä½¿ç”¨ç›®å½•åŠ è½½å™¨
    loader = DirectoryLoader(
        directory_path,
        glob="**/*.pdf",
        loader_cls=PyMuPDFLoader,
        show_progress=True,
        use_multithreading=True
    )
    
    documents = loader.load()
    print(f"æˆåŠŸåŠ è½½äº† {len(documents)} ä¸ªé¡µé¢ã€‚")
    
    # æ¸…æ´—å’Œåˆ†å—
    cleaned_documents = clean_document_content(documents)
    all_chunks = split_text_into_chunks(cleaned_documents)
    
    return all_chunks

def clean_document_content(documents: List[Document]) -> List[Document]:
    """
    å¯¹æ–‡æ¡£å†…å®¹è¿›è¡Œå¢å¼ºæ¸…æ´—
    """
    print("å¼€å§‹æ¸…æ´—æ–‡æ¡£å†…å®¹...")
    cleaned_documents = []
    
    for doc in documents:
        text = doc.page_content
        
        # é¢„å…ˆæ£€æµ‹é¡µé¢ç±»å‹
        page_type = 'content'
        if is_table_of_contents(text):
            page_type = 'toc'
            doc.metadata['page_type'] = 'table_of_contents'
        elif is_glossary_or_index(text):
            page_type = 'glossary'
            doc.metadata['page_type'] = 'glossary'
        elif is_reference_page(text):
            page_type = 'reference'
            doc.metadata['page_type'] = 'reference'
        else:
            doc.metadata['page_type'] = 'content'
        
        # æ¸…æ´—è§„åˆ™
        text = re.sub(r'(?<=[\u4e00-\u9fff])\s+(?=[\u4e00-\u9fff])', '', text)
        
        if page_type == 'content':
            text = re.sub(r'\(\s*(\d+\.\d+)\s*\)', r'ã€å…¬å¼\1ã€‘', text)
            text = re.sub(r'(?<![.\s])(å®šç†|å¼•ç†|è¯æ˜|æ¨è®º|å‘½é¢˜)\s*(\d+\.\d+)?(?!\s*\.)', 
                         r'\n\nã€\1\2ã€‘\n', text)
        
        if page_type == 'toc':
            text = re.sub(r'ã€(å®šç†|å¼•ç†|è¯æ˜|æ¨è®º|å‘½é¢˜)[^ã€‘]*ã€‘', '', text)
        
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        if page_type == 'content':
            ocr_fixes = {'BA': 'ä¸º', 'ME': 'ä½¿', 'sk': 'æ±‚'}
            for wrong, correct in ocr_fixes.items():
                text = re.sub(rf'\b{wrong}\b', correct, text)
            text = re.sub(r'([=â‰ˆâ‰ â‰¤â‰¥<>])', r' \1 ', text)
        
        text = re.sub(r' {2,}', ' ', text)
        text = re.sub(r'^\s*ç¬¬ \d+ ç« .*?\n', '', text, flags=re.MULTILINE)
        text = re.sub(r'^\s*\d+\s*$', '', text, flags=re.MULTILINE)
        
        doc.page_content = text.strip()
        
        # è¿‡æ»¤è¿‡çŸ­å†…å®¹
        if len(doc.page_content) > 100:
            cleaned_documents.append(doc)
    
    print(f"æ¸…æ´—å®Œæˆã€‚å‰©ä½™ {len(cleaned_documents)} ä¸ªæœ‰æ•ˆé¡µé¢ã€‚")
    return cleaned_documents

def is_table_of_contents(text: str) -> bool:
    """æ£€æµ‹ç›®å½•é¡µ"""
    dots_pattern = r'\.\s*\.\s*\.\s*\.\s*\d+'
    matches = re.findall(dots_pattern, text)
    if len(matches) > 5:
        return True
    
    toc_pattern = r'[\u4e00-\u9fff\w\s]+\.\s*\.\s*\.\s*\d+'
    toc_matches = re.findall(toc_pattern, text)
    if len(toc_matches) > 5:
        return True
    
    return False


def is_glossary_or_index(text: str) -> bool:
    """æ£€æµ‹è¯æ±‡è¡¨æˆ–ç´¢å¼•é¡µ"""
    pattern = r'[\w\u4e00-\u9fff]+\s+\d+(,\s*\d+|â€“\d+){3,}'
    matches = re.findall(pattern, text)
    if len(matches) > 10:
        return True
    
    word_number_pattern = r'\b[A-Za-z]+\s+[A-Za-z\s]+\d+'
    word_matches = re.findall(word_number_pattern, text)
    if len(word_matches) > 15:
        return True
    
    return False


def is_reference_page(text: str) -> bool:
    """æ£€æµ‹å‚è€ƒæ–‡çŒ®é¡µ"""
    reference_patterns = [
        r'\[\d+\]\s*[A-Z]',
        r'et\s+al\.',
        r'\([12]\d{3}\)\.',
        r'^[A-Z][a-z]+,\s*[A-Z\.]',
    ]
    
    matches = 0
    lines = text.split('\n')
    
    for line in lines:
        if any(re.search(p, line.strip()) for p in reference_patterns):
            matches += 1
    
    if len(lines) > 0:
        match_ratio = matches / len(lines)
        if matches > 5 or (match_ratio > 0.3 and len(lines) > 5):
            return True
    
    return False

def split_text_into_chunks(documents: List[Document]) -> List[Document]:
    """
    å°†æ–‡æ¡£åˆ‡åˆ†ä¸ºçŸ¥è¯†ç‰‡æ®µ
    """
    print("å¼€å§‹è¿›è¡Œæ–‡æœ¬åˆ†å—...")
    
    separators = [
        "\n\nã€å®šç†",
        "\n\nã€å¼•ç†",
        "\n\nã€è¯æ˜",
        "\n\nã€å…¬å¼",
        "\n\n\n",
        "\n\n",
        "ã€‚\n",
        "ï¼›\n",
        "\n",
        "ã€‚",
        "ï¼Œ",
        " ",
        ""
    ]
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=250,
        separators=separators,
        keep_separator=True,
        length_function=len,
    )
    
    all_chunks = text_splitter.split_documents(documents)
    all_chunks = post_process_chunks(all_chunks)
    
    print(f"æˆåŠŸå°†æ–‡æ¡£åˆ‡åˆ†ä¸º {len(all_chunks)} ä¸ªçŸ¥è¯†ç‰‡æ®µã€‚")
    return all_chunks


def post_process_chunks(chunks: List[Document]) -> List[Document]:
    """åå¤„ç†åˆ†å—ç»“æœ"""
    processed_chunks = []
    skip_next = False
    
    for i in range(len(chunks)):
        if skip_next:
            skip_next = False
            continue
        
        current_chunk = chunks[i]
        content = current_chunk.page_content.strip()
        
        # è¿‡æ»¤è¿‡çŸ­ç‰‡æ®µ
        if len(content) < 100:
            continue
        
        page_type = current_chunk.metadata.get('page_type', 'content')
        
        if page_type != 'content':
            current_chunk.metadata['is_special_page'] = True
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå¹¶
        if i < len(chunks) - 1 and page_type == 'content':
            next_chunk = chunks[i + 1]
            next_page_type = next_chunk.metadata.get('page_type', 'content')
            
            if next_page_type == 'content':
                if should_merge_with_next(content, next_chunk.page_content):
                    merged_content = content + "\n" + next_chunk.page_content
                    current_chunk.page_content = merged_content
                    skip_next = True
        
        processed_chunks.append(current_chunk)
    
    return processed_chunks


def should_merge_with_next(current_content: str, next_content: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸ä¸‹ä¸€å—åˆå¹¶"""
    if re.search(r'[=+\-*/]$', current_content.strip()):
        return True
    
    if re.search(r'[ï¼Œï¼›,;]$', current_content.strip()):
        return True
    
    if 'ã€è¯æ˜' in current_content and 'è¯æ¯•' not in current_content:
        if not next_content.strip().startswith('ã€'):
            return True
    
    if re.search(r'[ï¼ˆ([]$', current_content.strip()):
        return True
    
    return False

def analyze_chunk_quality(chunks: List[Document]) -> dict:
    """åˆ†æåˆ†å—è´¨é‡"""
    stats = {
        'total_chunks': len(chunks),
        'avg_length': 0,
        'min_length': float('inf'),
        'max_length': 0,
        'formula_chunks': 0,
        'theorem_chunks': 0,
        'proof_chunks': 0,
        'incomplete_chunks': 0,
        'toc_chunks': 0,
        'glossary_chunks': 0,
        'reference_chunks': 0,
        'content_chunks': 0,
    }
    
    total_length = 0
    incomplete_patterns = [
        r'[=+\-*/]$',
        r'[ï¼Œ,]$',
        r'[ï¼ˆ(]$',
    ]
    
    for chunk in chunks:
        content = chunk.page_content
        length = len(content)
        
        total_length += length
        stats['min_length'] = min(stats['min_length'], length)
        stats['max_length'] = max(stats['max_length'], length)
        
        page_type = chunk.metadata.get('page_type', 'content')
        if page_type == 'table_of_contents':
            stats['toc_chunks'] += 1
        elif page_type == 'glossary':
            stats['glossary_chunks'] += 1
        elif page_type == 'reference':
            stats['reference_chunks'] += 1
        else:
            stats['content_chunks'] += 1
            
            if 'ã€å…¬å¼' in content or re.search(r'[=â‰ˆâ‰ â‰¤â‰¥]', content):
                stats['formula_chunks'] += 1
            
            if 'ã€å®šç†' in content:
                stats['theorem_chunks'] += 1
            
            if 'ã€è¯æ˜' in content:
                stats['proof_chunks'] += 1
            
            if any(re.search(pattern, content.strip()) for pattern in incomplete_patterns):
                stats['incomplete_chunks'] += 1
    
    stats['avg_length'] = total_length / len(chunks) if chunks else 0
    
    return stats


def print_quality_report(stats: dict):
    """æ‰“å°è´¨é‡æŠ¥å‘Š"""
    print("\n" + "="*50)
    print(" ã€åˆ†å—è´¨é‡è¯¦ç»†æŠ¥å‘Šã€‘ ")
    print("="*50)
    
    print(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡:")
    print(f"  â€¢ æ€»ç‰‡æ®µæ•°: {stats['total_chunks']}")
    print(f"  â€¢ å¹³å‡é•¿åº¦: {stats['avg_length']:.0f} å­—ç¬¦")
    print(f"  â€¢ æœ€å°é•¿åº¦: {stats['min_length']} å­—ç¬¦")
    print(f"  â€¢ æœ€å¤§é•¿åº¦: {stats['max_length']} å­—ç¬¦")
    
    print(f"\nğŸ“„ é¡µé¢ç±»å‹åˆ†å¸ƒ:")
    print(f"  â€¢ æ­£æ–‡å†…å®¹: {stats['content_chunks']} "
          f"({stats['content_chunks']/stats['total_chunks']*100:.1f}%)")
    print(f"  â€¢ ç›®å½•é¡µ: {stats['toc_chunks']} "
          f"({stats['toc_chunks']/stats['total_chunks']*100:.1f}%)")
    print(f"  â€¢ è¯æ±‡è¡¨/ç´¢å¼•: {stats['glossary_chunks']} "
          f"({stats['glossary_chunks']/stats['total_chunks']*100:.1f}%)")
    print(f"  â€¢ å‚è€ƒæ–‡çŒ®: {stats['reference_chunks']} "
          f"({stats['reference_chunks']/stats['total_chunks']*100:.1f}%)")
    
    print("="*50)

def save_chunks_to_file(chunks: List[Document], output_dir: str = "./processed_chunks"):
    """ä¿å­˜çŸ¥è¯†å—åˆ°æ–‡ä»¶"""
    import json
    import pickle
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜ä¸ºJSON
    chunks_data = []
    for i, chunk in enumerate(chunks):
        chunks_data.append({
            'id': i,
            'content': chunk.page_content,
            'metadata': chunk.metadata
        })
    
    json_path = os.path.join(output_dir, "chunks.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(chunks_data, f, ensure_ascii=False, indent=2)
    print(f"âœ“ JSONæ ¼å¼å·²ä¿å­˜åˆ°: {json_path}")
    
    # ä¿å­˜ä¸ºPickle
    pickle_path = os.path.join(output_dir, "chunks.pkl")
    with open(pickle_path, 'wb') as f:
        pickle.dump(chunks, f)
    print(f"âœ“ Pickleæ ¼å¼å·²ä¿å­˜åˆ°: {pickle_path}")
    
    return json_path, pickle_path

if __name__ == "__main__":
    """
    ä»…ç”¨äºæµ‹è¯•ï¼Œç”Ÿäº§ç¯å¢ƒè¯·è°ƒç”¨ process_single_pdf() æˆ– process_directory()
    """
    print("æ–‡æ¡£å¤„ç†æ¨¡å— - æµ‹è¯•æ¨¡å¼")
    print("="*60)
    
    KB_PATH = "./knowledge_base"
    
    # æµ‹è¯•æ‰¹é‡å¤„ç†
    raw_documents = process_directory(KB_PATH)
    
    # è´¨é‡åˆ†æ
    stats = analyze_chunk_quality(raw_documents)
    print_quality_report(stats)
    
    # ä¿å­˜
    save_option = input("\næ˜¯å¦ä¿å­˜åˆ†å—ç»“æœï¼Ÿ(y/n): ").strip().lower()
    if save_option == 'y':
        save_chunks_to_file(raw_documents)