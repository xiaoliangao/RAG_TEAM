# quiz_module/evaluator.py

from typing import List, Dict, Any, Optional
from collections import Counter
import re


def grade_quiz(questions: List[Dict[str, Any]], user_answers_list: List[Optional[str]]) -> Dict[str, Any]:
    """
    ä¸»åˆ¤åˆ†å‡½æ•°ï¼ˆå¤„ç† st.form æäº¤çš„åŸå§‹ç­”æ¡ˆï¼‰
    
    Args:
        questions: åŸå§‹çš„é¢˜ç›®åˆ—è¡¨ (æ¥è‡ª st.session_state.quiz_questions)
        user_answers_list: ç”¨æˆ·åœ¨st.formä¸­æäº¤çš„ç­”æ¡ˆå­—ç¬¦ä¸²åˆ—è¡¨ (æ¥è‡ª st.radio)
                          ä¾‹å¦‚: ["A. é€‰é¡¹å†…å®¹", "True", None, "C. å…¶ä»–é€‰é¡¹"]
    
    Returns:
        åŒ…å«å¾—åˆ†å’Œè¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        {
            "total": int,
            "correct": int,
            "wrong": int,
            "unanswered": int,
            "score_percentage": float,
            "results": List[Dict],
            "knowledge_gaps": List[str],
            "statistics": Dict
        }
    """
    
    # 1. éªŒè¯è¾“å…¥
    if len(questions) != len(user_answers_list):
        raise ValueError(f"é¢˜ç›®æ•°é‡({len(questions)})ä¸ç­”æ¡ˆæ•°é‡({len(user_answers_list)})ä¸åŒ¹é…")
    
    # 2. å°†ç”¨æˆ·çš„ç­”æ¡ˆå­—ç¬¦ä¸²è½¬æ¢ä¸ºç´¢å¼•
    user_answers_map = {}
    unanswered_count = 0
    
    for i, q in enumerate(questions):
        user_ans_str = user_answers_list[i]
        
        if user_ans_str is None or user_ans_str == "":
            # ç”¨æˆ·æœªä½œç­”
            user_answers_map[i] = -1
            unanswered_count += 1
            continue
        
        try:
            # æ‰¾åˆ°ç”¨æˆ·é€‰æ‹©çš„å­—ç¬¦ä¸²åœ¨ options åˆ—è¡¨ä¸­çš„ç´¢å¼•
            user_ans_index = q['options'].index(user_ans_str)
            user_answers_map[i] = user_ans_index
        except ValueError:
            # å®¹é”™å¤„ç†ï¼šå°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤é€‰é¡¹å‰ç¼€ "A. "ï¼‰
            matched = False
            cleaned_user_ans = _clean_option_text(user_ans_str)
            
            for idx, opt in enumerate(q['options']):
                if _clean_option_text(opt) == cleaned_user_ans:
                    user_answers_map[i] = idx
                    matched = True
                    break
            
            if not matched:
                # å®åœ¨åŒ¹é…ä¸åˆ°ï¼Œæ ‡è®°ä¸ºæœªç­”
                user_answers_map[i] = -1
                unanswered_count += 1
                print(f"âš ï¸ è­¦å‘Šï¼šç¬¬{i+1}é¢˜çš„ç­”æ¡ˆæ— æ³•åŒ¹é…: '{user_ans_str}'")
    
    # 3. è°ƒç”¨æ ¸å¿ƒè®¡ç®—å‡½æ•°
    result = calculate_score(user_answers_map, questions)
    
    # 4. è¡¥å……æœªç­”é¢˜ç»Ÿè®¡
    result['unanswered'] = unanswered_count
    
    return result


def _clean_option_text(option: str) -> str:
    """
    æ¸…ç†é€‰é¡¹æ–‡æœ¬ï¼ˆç§»é™¤ A. B. True False ç­‰å‰ç¼€ï¼‰
    """
    # ç§»é™¤å¸¸è§å‰ç¼€
    option = re.sub(r'^[A-D]\.\s*', '', option)
    option = option.strip()
    return option


def calculate_score(user_answers: Dict[int, int], questions: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    è®¡ç®—æµ‹éªŒå¾—åˆ†ï¼ˆæ ¸å¿ƒç®—æ³•ï¼‰
    
    Args:
        user_answers: {question_index: selected_option_index}
                     -1 è¡¨ç¤ºæœªä½œç­”
        questions: é¢˜ç›®åˆ—è¡¨
    
    Returns:
        è¯¦ç»†çš„è¯„åˆ†æŠ¥å‘Šå­—å…¸
    """
    total = len(questions)
    correct = 0
    wrong = 0
    results = []
    wrong_types = []  # è®°å½•é”™é¢˜ç±»å‹ï¼ˆç”¨äºçŸ¥è¯†ç›²åŒºåˆ†æï¼‰
    
    for i, question in enumerate(questions):
        user_answer_index = user_answers.get(i, -1)
        correct_answer_index = question["correct_answer_index"]
        
        # åˆ¤æ–­æ­£è¯¯
        is_correct = (user_answer_index == correct_answer_index) and (user_answer_index != -1)
        
        if is_correct:
            correct += 1
        elif user_answer_index != -1:
            wrong += 1
            wrong_types.append(question.get('type', 'unknown'))
        
        # å­˜å‚¨è¯¦ç»†ç»“æœ
        results.append({
            "question_index": i,
            "question": question["question"],
            "type": question.get("type", "unknown"),
            "options": question["options"],
            "user_answer": user_answer_index,
            "correct_answer": correct_answer_index,
            "is_correct": is_correct,
            "is_unanswered": (user_answer_index == -1),
            "explanation": question["explanation"]
        })
    
    # è®¡ç®—å¾—åˆ†
    score_percentage = (correct / total * 100) if total > 0 else 0
    
    # ç»Ÿè®¡ä¿¡æ¯
    statistics = _calculate_statistics(results, wrong_types)
    
    # è¯†åˆ«çŸ¥è¯†ç›²åŒº
    knowledge_gaps = _identify_knowledge_gaps(results)
    
    return {
        "total": total,
        "correct": correct,
        "wrong": wrong,
        "unanswered": total - correct - wrong,
        "score_percentage": round(score_percentage, 2),
        "results": results,
        "knowledge_gaps": knowledge_gaps,
        "statistics": statistics
    }


def _calculate_statistics(results: List[Dict], wrong_types: List[str]) -> Dict[str, Any]:
    """
    è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    """
    # æŒ‰é¢˜å‹ç»Ÿè®¡
    choice_correct = sum(1 for r in results if r['type'] == 'choice' and r['is_correct'])
    choice_total = sum(1 for r in results if r['type'] == 'choice')
    
    boolean_correct = sum(1 for r in results if r['type'] == 'boolean' and r['is_correct'])
    boolean_total = sum(1 for r in results if r['type'] == 'boolean')
    
    # é”™é¢˜ç±»å‹åˆ†å¸ƒ
    wrong_type_count = Counter(wrong_types)
    
    return {
        "choice_accuracy": (choice_correct / choice_total * 100) if choice_total > 0 else 0,
        "boolean_accuracy": (boolean_correct / boolean_total * 100) if boolean_total > 0 else 0,
        "wrong_type_distribution": dict(wrong_type_count),
        "average_difficulty": _estimate_difficulty(results)
    }


def _estimate_difficulty(results: List[Dict]) -> str:
    """
    ä¼°è®¡æµ‹éªŒéš¾åº¦ï¼ˆåŸºäºæ­£ç¡®ç‡ï¼‰
    """
    correct_rate = sum(1 for r in results if r['is_correct']) / len(results) if results else 0
    
    if correct_rate >= 0.8:
        return "ç®€å•"
    elif correct_rate >= 0.5:
        return "ä¸­ç­‰"
    else:
        return "å›°éš¾"


def _identify_knowledge_gaps(results: List[Dict]) -> List[str]:
    """
    è¯†åˆ«çŸ¥è¯†ç›²åŒºï¼ˆä»é”™é¢˜ä¸­æå–å…³é”®æ¦‚å¿µï¼‰
    
    è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œåç»­å¯ä»¥ç»“åˆNLPè¿›è¡Œæ›´ç²¾ç¡®çš„æå–
    """
    gaps = []
    wrong_questions = [r for r in results if not r['is_correct'] and not r['is_unanswered']]
    
    if not wrong_questions:
        return []
    
    # ç®€å•çš„å…³é”®è¯æå–ï¼ˆåç»­å¯ä»¥ä½¿ç”¨çŸ¥è¯†å›¾è°±ä¼˜åŒ–ï¼‰
    keywords = set()
    common_terms = ['æ˜¯', 'çš„', 'äº†', 'åœ¨', 'å’Œ', 'ä¸', 'æˆ–', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'ï¼Ÿ', 'ï¼Œ', 'ã€‚']
    
    for item in wrong_questions:
        question = item['question']
        # æå–é—®é¢˜ä¸­çš„å…³é”®è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        words = [w for w in question.split() if w not in common_terms and len(w) > 2]
        keywords.update(words[:3])  # æœ€å¤šå–å‰3ä¸ªå…³é”®è¯
    
    # æ„å»ºçŸ¥è¯†ç›²åŒºæè¿°
    if keywords:
        gaps.append(f"æ¶‰åŠä»¥ä¸‹æ¦‚å¿µï¼š{', '.join(list(keywords)[:5])}")
    
    # æŒ‰é¢˜å‹åˆ†ç±»
    wrong_by_type = {}
    for item in wrong_questions:
        q_type = "é€‰æ‹©é¢˜" if item['type'] == 'choice' else "åˆ¤æ–­é¢˜"
        wrong_by_type[q_type] = wrong_by_type.get(q_type, 0) + 1
    
    for q_type, count in wrong_by_type.items():
        gaps.append(f"{q_type}é”™è¯¯è¾ƒå¤šï¼ˆ{count}é¢˜ï¼‰")
    
    return gaps


def get_performance_level(score_percentage: float) -> Dict[str, str]:
    """
    æ ¹æ®å¾—åˆ†è·å–è¡¨ç°ç­‰çº§
    
    Returns:
        {"level": str, "emoji": str, "color": str, "message": str}
    """
    if score_percentage >= 90:
        return {
            "level": "ä¼˜ç§€",
            "emoji": "ğŸ†",
            "color": "green",
            "message": "å‡ºè‰²çš„è¡¨ç°ï¼ä½ å¯¹è¿™éƒ¨åˆ†å†…å®¹æŒæ¡å¾—éå¸¸å¥½ï¼"
        }
    elif score_percentage >= 80:
        return {
            "level": "è‰¯å¥½",
            "emoji": "ğŸ¥ˆ",
            "color": "blue",
            "message": "ä¸é”™çš„æˆç»©ï¼ç»§ç»­ä¿æŒï¼Œä½ ç¦»ä¼˜ç§€åªå·®ä¸€æ­¥äº†ï¼"
        }
    elif score_percentage >= 70:
        return {
            "level": "ä¸­ç­‰",
            "emoji": "ğŸ¥‰",
            "color": "orange",
            "message": "è¿˜ä¸é”™ï¼å†å¤šç»ƒä¹ ä¸€ä¸‹å°±èƒ½æ›´ä¸Šä¸€å±‚æ¥¼ï¼"
        }
    elif score_percentage >= 60:
        return {
            "level": "åŠæ ¼",
            "emoji": "ğŸ“˜",
            "color": "yellow",
            "message": "åŸºç¡€è¿˜å¯ä»¥ï¼Œå»ºè®®åŠ å¼ºè–„å¼±ç¯èŠ‚çš„å­¦ä¹ ã€‚"
        }
    else:
        return {
            "level": "éœ€åŠ å¼º",
            "emoji": "ğŸ“•",
            "color": "red",
            "message": "ä¸è¦æ°”é¦ï¼æ‰¾åˆ°çŸ¥è¯†ç›²åŒºï¼Œç³»ç»Ÿå­¦ä¹ åä¸€å®šèƒ½è¿›æ­¥ï¼"
        }


def format_detailed_results(report_data: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–è¯¦ç»†çš„ç­”é¢˜ç»“æœï¼ˆç”¨äºå¯¼å‡ºæˆ–æ˜¾ç¤ºï¼‰
    
    Returns:
        Markdownæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š
    """
    results = report_data['results']
    
    output = []
    output.append("# æµ‹éªŒè¯¦ç»†ç»“æœ\n")
    output.append(f"## æ€»ä½“è¡¨ç°")
    output.append(f"- æ€»é¢˜æ•°: {report_data['total']}")
    output.append(f"- ç­”å¯¹: {report_data['correct']}")
    output.append(f"- ç­”é”™: {report_data['wrong']}")
    output.append(f"- æœªç­”: {report_data['unanswered']}")
    output.append(f"- å¾—åˆ†: {report_data['score_percentage']}%\n")
    
    # ç­”é¢˜è¯¦æƒ…
    output.append("## ç­”é¢˜è¯¦æƒ…\n")
    
    for i, result in enumerate(results, 1):
        status = "âœ… æ­£ç¡®" if result['is_correct'] else ("â­• æœªç­”" if result['is_unanswered'] else "âŒ é”™è¯¯")
        output.append(f"### ç¬¬ {i} é¢˜ - {status}")
        output.append(f"**é—®é¢˜:** {result['question']}\n")
        
        output.append("**é€‰é¡¹:**")
        for idx, opt in enumerate(result['options']):
            marker = ""
            if idx == result['user_answer']:
                marker = " ğŸ‘ˆ æ‚¨çš„ç­”æ¡ˆ"
            if idx == result['correct_answer']:
                marker += " âœ… æ­£ç¡®ç­”æ¡ˆ"
            output.append(f"- {opt}{marker}")
        
        output.append(f"\n**è§£æ:** {result['explanation']}\n")
        output.append("---\n")
    
    return "\n".join(output)