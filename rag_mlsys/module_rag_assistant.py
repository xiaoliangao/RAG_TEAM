# module_rag_assistant.py
import streamlit as st
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever
from threading import Thread
from typing import List, Dict, Tuple
import json
from datetime import datetime
import hashlib
import os
from llm_client import chat_completion, LLMClientError
import torch

VECTOR_DB_PATH = "./vector_db"
EMBEDDING_MODEL_NAME = "./models/bge-large-zh-v1.5"
LLM_MODEL_PATH = "./models/Qwen2.5-7B-Instruct"
FEEDBACK_DB_PATH = "./feedback_db"

GENERATION_CONFIG = {
    "max_new_tokens": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "repetition_penalty": 1.1,
    "do_sample": True,
}

FEW_SHOT_EXAMPLES = [
    {
        "question": "ä»€ä¹ˆæ˜¯åå‘ä¼ æ’­ç®—æ³•ï¼Ÿ",
        "answer": """**åå‘ä¼ æ’­ç®—æ³•**æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ï¼Œç”¨äºé«˜æ•ˆè®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚

**æ ¸å¿ƒæµç¨‹ï¼š**

1. **å‰å‘ä¼ æ’­**
   - è¾“å…¥æ•°æ®é€å±‚é€šè¿‡ç½‘ç»œ
   - æ¯å±‚è¿›è¡Œçº¿æ€§å˜æ¢å’Œæ¿€æ´»å‡½æ•°è®¡ç®—
   - æœ€ç»ˆå¾—åˆ°é¢„æµ‹è¾“å‡º

2. **è®¡ç®—æŸå¤±**
   - å¯¹æ¯”é¢„æµ‹å€¼ä¸çœŸå®æ ‡ç­¾
   - ä½¿ç”¨æŸå¤±å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µã€MSEï¼‰é‡åŒ–è¯¯å·®

3. **åå‘ä¼ æ’­**
   - ä»è¾“å‡ºå±‚å¼€å§‹ï¼Œå‘è¾“å…¥å±‚é€å±‚ä¼ é€’
   - åˆ©ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
   - âˆ‚L/âˆ‚w = âˆ‚L/âˆ‚y Ã— âˆ‚y/âˆ‚w

4. **å‚æ•°æ›´æ–°**
   - ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨æ›´æ–°æƒé‡
   - w_new = w_old - learning_rate Ã— gradient

**å…³é”®ä¼˜åŠ¿ï¼š** é€šè¿‡ç¼“å­˜å‰å‘ä¼ æ’­çš„ä¸­é—´ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œå¤§å¹…æå‡è®­ç»ƒæ•ˆç‡ã€‚"""
    },
    {
        "question": "Batch Normalizationå¦‚ä½•å·¥ä½œï¼Ÿ",
        "answer": """**Batch Normalizationï¼ˆæ‰¹å½’ä¸€åŒ–ï¼‰**æ˜¯ä¸€ç§å¼ºå¤§çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œèƒ½æ˜¾è‘—æ”¹å–„æ·±åº¦ç½‘ç»œè®­ç»ƒã€‚

**å·¥ä½œæœºåˆ¶ï¼š**

1. **æ ‡å‡†åŒ–**
   - å¯¹æ¯ä¸ªbatchçš„æ¿€æ´»å€¼è¿›è¡Œæ ‡å‡†åŒ–
   - ä½¿å…¶å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1
   - x_norm = (x - Î¼_batch) / âˆš(ÏƒÂ²_batch + Îµ)

2. **ç¼©æ”¾å’Œå¹³ç§»**
   - å¼•å…¥å¯å­¦ä¹ å‚æ•°Î³ï¼ˆscaleï¼‰å’ŒÎ²ï¼ˆshiftï¼‰
   - y = Î³ Ã— x_norm + Î²
   - å…è®¸ç½‘ç»œæ¢å¤åŸå§‹è¡¨ç¤ºèƒ½åŠ›

**ä¸»è¦ä¼˜åŠ¿ï¼š**

- **åŠ é€Ÿæ”¶æ•›**ï¼šç¨³å®šæ¿€æ´»åˆ†å¸ƒï¼Œå…è®¸ä½¿ç”¨æ›´å¤§å­¦ä¹ ç‡
- **å‡å°‘æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸**ï¼šè§„èŒƒåŒ–æ¿€æ´»å€¼èŒƒå›´
- **æ­£åˆ™åŒ–æ•ˆåº”**ï¼šbatché—´çš„éšæœºæ€§äº§ç”Ÿç±»ä¼¼dropoutçš„æ•ˆæœ
- **é™ä½å¯¹åˆå§‹åŒ–çš„æ•æ„Ÿåº¦**ï¼šä½¿ç½‘ç»œæ›´å®¹æ˜“è®­ç»ƒ

**åº”ç”¨åœºæ™¯ï¼š** é€šå¸¸æ”¾ç½®åœ¨çº¿æ€§å±‚ä¹‹åã€æ¿€æ´»å‡½æ•°ä¹‹å‰ã€‚"""
    }
]


class EnsembleRetriever:
    """æ··åˆæ£€ç´¢å™¨ï¼šå‘é‡æ£€ç´¢ + BM25"""
    def __init__(self, retrievers, weights=None):
        self.retrievers = retrievers
        self.weights = weights or [1.0] * len(retrievers)

    def invoke(self, query: str) -> List[Document]:
        all_docs = []
        for retriever, w in zip(self.retrievers, self.weights):
            try:
                docs = retriever.invoke(query)
            except Exception:
                docs = retriever.get_relevant_documents(query)
            all_docs.extend(docs * int(w * 10))

        unique_docs = {d.page_content: d for d in all_docs}
        return list(unique_docs.values())

@st.cache_resource
def load_retriever(db_path, model_name):
    """åŠ è½½æ£€ç´¢å¼•æ“"""
    with st.spinner("æ­£åœ¨åŠ è½½æ£€ç´¢å¼•æ“..."):
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        embedding_model = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        db = Chroma(
            persist_directory=db_path,
            embedding_function=embedding_model
        )
        
        vector_retriever = db.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 6}
        )
        
        try:
            all_data = db.get()
            if all_data and all_data.get('documents'):
                docs = [Document(page_content=doc, metadata=meta) 
                       for doc, meta in zip(all_data['documents'], 
                                           all_data.get('metadatas', [{}]*len(all_data['documents'])))]
                bm25_retriever = BM25Retriever.from_documents(docs)
                bm25_retriever.k = 6
                
                ensemble_retriever = EnsembleRetriever(
                    retrievers=[vector_retriever, bm25_retriever],
                    weights=[0.6, 0.4]
                )
                st.success("âœ“ æ··åˆæ£€ç´¢å™¨å·²å°±ç»ª")
                return ensemble_retriever
        except Exception as e:
            st.warning(f"ä½¿ç”¨å‘é‡æ£€ç´¢")
        
        return vector_retriever


@st.cache_resource
def generate_queries(original_query, num_queries=2):
    """æ™ºèƒ½æŸ¥è¯¢æ‰©å±•"""
    queries = [original_query]
    
    # è¡¥å……ç–‘é—®è¯
    if not original_query.startswith(("ä»€ä¹ˆ", "å¦‚ä½•", "ä¸ºä»€ä¹ˆ", "è¯·é—®", "èƒ½å¦", "æ€ä¹ˆ")):
        queries.append(f"ä»€ä¹ˆæ˜¯{original_query}")
    
    # æ·»åŠ è§£é‡Šæ€§æŸ¥è¯¢
    if "è§£é‡Š" not in original_query and "ä»‹ç»" not in original_query:
        queries.append(f"è¯·è§£é‡Š{original_query}")
    
    # æ·»åŠ é¢†åŸŸå‰ç¼€
    domain_keywords = ["æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "ç®—æ³•"]
    has_domain = any(kw in original_query for kw in domain_keywords)
    
    if not has_domain and len(queries) < num_queries + 1:
        queries.append(f"æ·±åº¦å­¦ä¹ ä¸­çš„{original_query}")
    
    return queries[:num_queries + 1]


def smart_context_selection(docs, query, max_docs=4):
    """æ™ºèƒ½ä¸Šä¸‹æ–‡é€‰æ‹©ï¼šå¤šç»´åº¦è¯„åˆ†"""
    if len(docs) <= max_docs:
        return docs
    
    query_terms = set(query.lower().split())
    
    scored_docs = []
    for doc in docs:
        content_lower = doc.page_content.lower()
        
        # 1. å…³é”®è¯åŒ¹é…å¾—åˆ†
        keyword_score = sum(1 for term in query_terms if term in content_lower)
        
        # 2. æ–‡æ¡£é•¿åº¦å¾—åˆ†ï¼ˆæ›´å®Œæ•´çš„ä¿¡æ¯ï¼‰
        length_score = min(len(doc.page_content) / 1000, 2.0)
        
        # 3. æ–‡æ¡£å¤šæ ·æ€§ï¼ˆé¿å…é‡å¤ï¼‰
        diversity_score = 1.0
        
        total_score = keyword_score * 2 + length_score + diversity_score
        scored_docs.append((total_score, doc))
    
    scored_docs.sort(reverse=True, key=lambda x: x[0])
    return [doc for _, doc in scored_docs[:max_docs]]


def extract_dialogue_context(messages, max_history=3):
    """æå–å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡"""
    if len(messages) < 3:
        return None
    
    recent_messages = messages[-(2*max_history):]
    
    context_parts = []
    for i in range(0, len(recent_messages), 2):
        if i+1 < len(recent_messages):
            user_msg = recent_messages[i]["content"][:150]
            assistant_msg = recent_messages[i+1]["content"][:150]
            context_parts.append(f"Q: {user_msg}\nA: {assistant_msg}")
    
    return "\n\n".join(context_parts) if context_parts else None


def retrieve_with_enhancements(retriever, query, k=4, enable_expansion=True):
    """å¢å¼ºæ£€ç´¢"""
    try:
        all_docs = []
        seen_content = set()
        
        if enable_expansion:
            queries = generate_queries(query, num_queries=2)
        else:
            queries = [query]
        
        for q in queries:
            docs = retriever.invoke(q)
            
            for doc in docs:
                content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
                if content_hash not in seen_content:
                    all_docs.append(doc)
                    seen_content.add(content_hash)
        
        final_docs = smart_context_selection(all_docs, query, max_docs=k)
        
        context_parts = []
        sources = []
        
        for i, doc in enumerate(final_docs, 1):
            source = doc.metadata.get('source', 'Unknown')
            page = doc.metadata.get('page', 'N/A')
            
            context_parts.append(f"[æ–‡æ¡£ {i}]\n{doc.page_content}")
            sources.append(f"{source} (é¡µç : {page})")
        
        context = "\n\n".join(context_parts)
        
        return context, sources, final_docs
        
    except Exception as e:
        st.error(f"æ£€ç´¢å‡ºé”™: {e}")
        return "", [], []

def build_enhanced_prompt(context, question, dialogue_history=None, 
                         use_fewshot=True, use_multi_turn=True):
    """æ„å»ºä¼˜åŒ–çš„prompt"""
    
    system_prompt = """ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ ä¸“å®¶æ•™å¸ˆã€‚ä½ çš„ä½¿å‘½æ˜¯å¸®åŠ©å­¦ä¹ è€…æ·±å…¥ç†è§£å¤æ‚çš„æŠ€æœ¯æ¦‚å¿µã€‚

**æ•™å­¦åŸåˆ™ï¼š**

1. **å‡†ç¡®æ€§æ˜¯åŸºç¡€**
   - ä¸¥æ ¼åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™å›ç­”
   - ä¸ç¼–é€ æˆ–è‡†æµ‹è¶…å‡ºèµ„æ–™èŒƒå›´çš„å†…å®¹
   - é‡åˆ°èµ„æ–™ä¸è¶³æ—¶ï¼Œè¯šå®è¯´æ˜å¹¶å»ºè®®æŸ¥é˜…æ–¹å‘

2. **ç»“æ„åŒ–è¡¨è¾¾**
   - ä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜å’Œå±‚æ¬¡ç»„ç»‡å†…å®¹
   - å…ˆæ¦‚è¿°æ ¸å¿ƒæ¦‚å¿µï¼Œå†å±•å¼€ç»†èŠ‚
   - å–„ç”¨**åŠ ç²—**ã€ç¼–å·åˆ—è¡¨ã€åˆ†ç‚¹è¯´æ˜

3. **æ·±å…¥æµ…å‡º**
   - å¤æ‚æ¦‚å¿µå…ˆç»™å‡ºç›´è§‚è§£é‡Š
   - é€‚æ—¶ä½¿ç”¨ç±»æ¯”å’Œå®ä¾‹å¸®åŠ©ç†è§£
   - å¿…è¦æ—¶æŒ‡å‡ºæ•°å­¦åŸç†ï¼Œä½†ä¿æŒå¯è¯»æ€§

4. **ç†è®ºè”ç³»å®è·µ**
   - è¯´æ˜æ¦‚å¿µçš„å®é™…åº”ç”¨åœºæ™¯
   - æŒ‡å‡ºå¸¸è§è¯¯åŒºå’Œæ³¨æ„äº‹é¡¹
   - æä¾›è¿›ä¸€æ­¥å­¦ä¹ çš„æ–¹å‘

5. **å¯¹è¯è¿è´¯æ€§**ï¼ˆå¤šè½®å¯¹è¯æ—¶ï¼‰
   - å‚è€ƒä¹‹å‰è®¨è®ºçš„å†…å®¹
   - é€æ­¥æ·±å…¥ï¼Œé¿å…é‡å¤
   - å›ç­”æ—¶å‘¼åº”å­¦ä¹ è€…çš„é—®é¢˜è„‰ç»œ

**å›ç­”é£æ ¼ï¼š** ä¸“ä¸šè€Œå‹å¥½ï¼Œåƒä¸€ä½è€å¿ƒçš„å¯¼å¸ˆä¸å­¦ç”Ÿé¢å¯¹é¢äº¤æµã€‚"""

    # Few-shotç¤ºä¾‹
    fewshot_text = ""
    if use_fewshot:
        fewshot_text = "\n\n**å‚è€ƒç¤ºä¾‹ï¼š**\n"
        for i, example in enumerate(FEW_SHOT_EXAMPLES[:2], 1):
            fewshot_text += f"\nã€ç¤ºä¾‹ {i}ã€‘\né—®ï¼š{example['question']}\nç­”ï¼š{example['answer'][:300]}...\n"
    
    # å¯¹è¯å†å²
    history_section = ""
    if use_multi_turn and dialogue_history:
        history_section = f"\n\n**ä¹‹å‰çš„å¯¹è¯ï¼š**\n{dialogue_history}\n"
    
    user_message = f"""{fewshot_text}

**å‚è€ƒèµ„æ–™ï¼š**
{context}{history_section}

---

**å½“å‰é—®é¢˜ï¼š** {question}

è¯·åŸºäºå‚è€ƒèµ„æ–™ï¼Œæä¾›ä¸€ä¸ªä¸“ä¸šã€å‡†ç¡®ä¸”æ˜“äºç†è§£çš„å›ç­”ã€‚"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ]
    
    return messages

def save_feedback(question, answer, feedback_type, comment=""):
    """ä¿å­˜ç”¨æˆ·åé¦ˆ"""
    try:
        os.makedirs(FEEDBACK_DB_PATH, exist_ok=True)
        
        feedback_data = {
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "answer": answer[:200],
            "type": feedback_type,
            "comment": comment
        }
        
        feedback_file = os.path.join(
            FEEDBACK_DB_PATH,
            f"feedback_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}.json"
        )
        
        with open(feedback_file, 'w', encoding='utf-8') as f:
            json.dump(feedback_data, f, ensure_ascii=False, indent=2)
        
        return True
    except Exception as e:
        st.error(f"ä¿å­˜åé¦ˆå¤±è´¥: {e}")
        return False


def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ """
    with st.sidebar:
        st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")
        
        st.divider()
        
        st.subheader("ğŸ¯ æ£€ç´¢ä¼˜åŒ–")
        
        enable_query_expansion = st.checkbox(
            "æŸ¥è¯¢æ‰©å±•",
            value=True,
            help="è‡ªåŠ¨ç”Ÿæˆç›¸å…³æŸ¥è¯¢ï¼Œæé«˜æ£€ç´¢è¦†ç›–ç‡"
        )
        
        enable_multi_turn = st.checkbox(
            "å¤šè½®å¯¹è¯ä¼˜åŒ–",
            value=True,
            help="åœ¨å¯¹è¯ä¸­è€ƒè™‘å†å²ä¸Šä¸‹æ–‡"
        )
        
        if enable_multi_turn:
            max_history_turns = st.slider(
                "å¯¹è¯å†å²è½®æ•°",
                min_value=1,
                max_value=5,
                value=3
            )
        else:
            max_history_turns = 0
        
        use_fewshot = st.checkbox(
            "Few-shotç¤ºä¾‹",
            value=True,
            help="åœ¨promptä¸­åŒ…å«ç¤ºä¾‹å›ç­”"
        )
        
        st.divider()
        
        st.subheader("ğŸ›ï¸ ç”Ÿæˆå‚æ•°")
        
        temperature = st.slider(
            "Temperature",
            min_value=0.1,
            max_value=2.0,
            value=0.7,
            step=0.1,
            help="æ§åˆ¶å›ç­”çš„åˆ›é€ æ€§"
        )
        
        top_p = st.slider(
            "Top-p",
            min_value=0.1,
            max_value=1.0,
            value=0.9,
            step=0.05
        )
        
        max_tokens = st.slider(
            "Max Tokens",
            min_value=512,
            max_value=4096,
            value=2048,
            step=256
        )
        
        GENERATION_CONFIG['temperature'] = temperature
        GENERATION_CONFIG['top_p'] = top_p
        GENERATION_CONFIG['max_new_tokens'] = max_tokens
        
        st.divider()
        
        st.subheader("ğŸ” æ£€ç´¢é…ç½®")
        
        k_documents = st.slider(
            "æ£€ç´¢æ–‡æ¡£æ•°",
            min_value=2,
            max_value=8,
            value=5
        )
        
        st.divider()
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ—‘ï¸ æ¸…ç©º", use_container_width=True):
                st.session_state.messages = []
                st.rerun()
        
        with col2:
            if st.button("ğŸ”„ é‡ç”Ÿæˆ", use_container_width=True):
                if len(st.session_state.get('messages', [])) >= 2:
                    st.session_state.messages = st.session_state.messages[:-1]
                    st.rerun()
        
        return (k_documents, enable_query_expansion, enable_multi_turn, 
            max_history_turns, use_fewshot,temperature, top_p, max_tokens)

def main():
    st.set_page_config(
        page_title="ML/DL AIåŠ©æ•™",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ¤– ML/DL AIæ™ºèƒ½åŠ©æ•™")
    st.caption("åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    
    (k_documents, enable_query_expansion, enable_multi_turn, max_history_turns, use_fewshot,
        temperature, top_p, max_tokens) = render_sidebar()
    
    # åˆå§‹åŒ–æ¨¡å‹
    if 'models_loaded' not in st.session_state:
        with st.status("æ­£åœ¨åˆå§‹åŒ–...", expanded=True) as status:
            try:
                st.write("ğŸ“¥ åŠ è½½æ£€ç´¢å¼•æ“...")
                retriever = load_retriever(VECTOR_DB_PATH, EMBEDDING_MODEL_NAME)
                st.session_state.retriever = retriever
                
                st.write("ğŸ§  ä½¿ç”¨è¿œç¨‹ LLMï¼ˆAPI è°ƒç”¨ï¼‰...")
                
                st.session_state.models_loaded = True
                status.update(label="âœ… ç³»ç»Ÿå‡†å¤‡å°±ç»ª", state="complete", expanded=False)
                
            except Exception as e:
                status.update(label="âŒ åˆå§‹åŒ–å¤±è´¥", state="error", expanded=True)
                st.error(f"é”™è¯¯: {e}")
                st.stop()
    
    # åˆå§‹åŒ–èŠå¤©è®°å½•
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # æ¬¢è¿æ¶ˆæ¯
    if len(st.session_state.messages) == 0:
        with st.chat_message("assistant"):
            st.markdown("""
ğŸ‘‹ æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„ML/DLå­¦ä¹ åŠ©æ•™ã€‚

**æˆ‘èƒ½å¸®æ‚¨ï¼š**
- ğŸ“š è§£é‡Šæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¦‚å¿µ
- ğŸ” åŸºäºæ•™ææä¾›å‡†ç¡®çš„æŠ€æœ¯è§£ç­”
- ğŸ’¡ æä¾›å­¦ä¹ å»ºè®®å’ŒçŸ¥è¯†ç‚¹æ¢³ç†
- ğŸ—£ï¸ è¿›è¡Œè¿è´¯çš„å¤šè½®å¯¹è¯äº¤æµ

**åŠŸèƒ½ç‰¹æ€§ï¼š**
- ğŸ¯ **æ™ºèƒ½æ£€ç´¢** - æ··åˆå‘é‡æ£€ç´¢+å…³é”®è¯åŒ¹é…
- ğŸ”„ **æŸ¥è¯¢ä¼˜åŒ–** - è‡ªåŠ¨æ‰©å±•æŸ¥è¯¢æé«˜è¦†ç›–ç‡
- ğŸ’¬ **å¯¹è¯è®°å¿†** - ç†è§£ä¸Šä¸‹æ–‡ï¼Œè¿è´¯äº¤æµ
- ğŸ“– **å¼•ç”¨æ¥æº** - æ¯ä¸ªå›ç­”éƒ½æ ‡æ³¨å‚è€ƒèµ„æ–™

**æé—®å»ºè®®ï¼š**
- "ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ"
- "å¯¹æ¯”Adamå’ŒSGDä¼˜åŒ–å™¨çš„ä¼˜ç¼ºç‚¹"
- "å¦‚ä½•è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Ÿ"
- "ç»§ç»­è§£é‡Šå…¶ä¸­çš„æ•°å­¦åŸç†"ï¼ˆå¤šè½®å¯¹è¯ï¼‰

ç°åœ¨å°±å¼€å§‹æé—®å§ï¼ ğŸš€
            """)
    
    # æ˜¾ç¤ºèŠå¤©å†å²
    for i, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            if message["role"] == "assistant":
                if "sources" in message:
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        with st.expander("ğŸ“š æŸ¥çœ‹å¼•ç”¨æ¥æº"):
                            for j, source in enumerate(message["sources"], 1):
                                st.text(f"{j}. {source}")
                    
                    with col2:
                        st.caption("åé¦ˆ")
                        col_like, col_dislike = st.columns(2)
                        
                        with col_like:
                            if st.button("ğŸ‘", key=f"like_{i}"):
                                save_feedback(
                                    message.get("question", ""),
                                    message["content"],
                                    "helpful"
                                )
                                st.toast("æ„Ÿè°¢åé¦ˆï¼")
                        
                        with col_dislike:
                            if st.button("ğŸ‘", key=f"dislike_{i}"):
                                save_feedback(
                                    message.get("question", ""),
                                    message["content"],
                                    "unhelpful"
                                )
                                st.toast("æ„Ÿè°¢åé¦ˆï¼")
    
    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if user_question := st.chat_input("ğŸ’­ è¯·è¾“å…¥é—®é¢˜..."):
        st.session_state.messages.append({
            "role": "user",
            "content": user_question
        })
        
        with st.chat_message("user"):
            st.markdown(user_question)
        
        # ç”Ÿæˆå›ç­”
        with st.chat_message("assistant"):
            status_container = st.empty()
            
            # æ£€ç´¢
            with status_container.status("ğŸ” æ­£åœ¨æ£€ç´¢...", expanded=False) as status:
                context, sources, docs = retrieve_with_enhancements(
                    st.session_state.retriever,
                    user_question,
                    k=k_documents,
                    enable_expansion=enable_query_expansion
                )
                
                status_info = []
                if enable_query_expansion:
                    status_info.append("âœ“ æŸ¥è¯¢æ‰©å±•")
                status_info.append("âœ“ æ··åˆæ£€ç´¢")
                
                st.write(", ".join(status_info))
                st.write(f"âœ“ æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
            
            if not docs:
                st.error("âŒ æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
                full_response = "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•æ¢ä¸ªæ–¹å¼æé—®ã€‚"
                st.markdown(full_response)
            else:
                # æå–å¯¹è¯å†å²
                dialogue_history = None
                if enable_multi_turn and len(st.session_state.messages) > 2:
                    with status_container.status("ğŸ’­ åˆ†æå¯¹è¯...", expanded=False):
                        dialogue_history = extract_dialogue_context(
                            st.session_state.messages[:-1],
                            max_history=max_history_turns
                        )
                        if dialogue_history:
                            st.write(f"âœ“ åŒ…å« {max_history_turns} è½®å¯¹è¯")
                
                # ç”Ÿæˆå›ç­”
                with status_container.status("âœï¸ æ­£åœ¨ç”Ÿæˆ...", expanded=False):
                    messages = build_enhanced_prompt(
                        context,
                        user_question,
                        dialogue_history=dialogue_history,
                        use_fewshot=use_fewshot,
                        use_multi_turn=enable_multi_turn
                    )

                    response_placeholder = st.empty()

                    try:
                        # ç›´æ¥è°ƒç”¨è¿œç¨‹ LLMï¼Œä¸€æ¬¡æ€§æ‹¿åˆ°å®Œæ•´å›ç­”
                        full_response = chat_completion(
                            messages,
                            temperature=0.7,   # ä½ å¯ä»¥ç”¨ç•Œé¢ä¸Šçš„å‚æ•°æ›¿ä»£
                            max_tokens=1024
                        )
                        response_placeholder.markdown(full_response)
                        status_container.empty()
                    except LLMClientError as e:
                        st.error(f"âŒ ç”Ÿæˆå‡ºé”™: {e}")
                        full_response = "æŠ±æ­‰ï¼Œç”Ÿæˆæ—¶é‡åˆ°é—®é¢˜ã€‚"
                        response_placeholder.markdown(full_response)
                
                # æ˜¾ç¤ºæ¥æºå’Œåé¦ˆ
                if sources:
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        with st.expander("ğŸ“š æŸ¥çœ‹å¼•ç”¨æ¥æº"):
                            for i, source in enumerate(sources, 1):
                                st.text(f"{i}. {source}")
                    
                    with col2:
                        st.caption("åé¦ˆ")
                        col_like, col_dislike = st.columns(2)
                        
                        with col_like:
                            if st.button("ğŸ‘", key=f"new_like"):
                                save_feedback(user_question, full_response, "helpful")
                                st.toast("æ„Ÿè°¢åé¦ˆï¼")
                        
                        with col_dislike:
                            if st.button("ğŸ‘", key=f"new_dislike"):
                                save_feedback(user_question, full_response, "unhelpful")
                                st.toast("æ„Ÿè°¢åé¦ˆï¼")
            
            # ä¿å­˜åˆ°å†å²
            st.session_state.messages.append({
                "role": "assistant",
                "content": full_response,
                "sources": sources,
                "question": user_question
            })
            
            st.rerun()


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        st.error(f"åº”ç”¨è¿è¡Œå‡ºé”™: {e}")
        import traceback
        st.code(traceback.format_exc())